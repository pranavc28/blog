<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Tinkering with Generative UI | Pranav on a Tangent</title>
<meta name="keywords" content="">
<meta name="description" content="&ldquo;Form and function should be one, joined in a spiritual union&rdquo; - Frank Lloyd Wright
Purpose
This blog explores how I fine tuned Qwen, Qwen/Qwen3-30B-A3B, a chinese open source model producing React code for a fraction of the cost of any of the top-tier AI research lab. I used Tinker to fine tune my own LLM, a product released by the team at Thinking Machines Lab.
It did a great job, and produced generally usable React code to the level that I expected it to from an online dataset.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/blog/posts/generative-ui-tinker/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.svg">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/generative-ui-tinker/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üç∫</text></svg>">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\[', right: '\\]', display: true},
              {left: '\\(', right: '\\)', display: false}
          ],
          throwOnError : false
        });
    });
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Pranav on a Tangent (Alt + H)">Pranav on a Tangent</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Tinkering with Generative UI
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-11-14 07:07:07 +0100 +0100'>November 14, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><strong>&ldquo;Form and function should be one, joined in a spiritual union&rdquo; - Frank Lloyd Wright</strong></p>
<h2 id="purpose">Purpose<a hidden class="anchor" aria-hidden="true" href="#purpose">#</a></h2>
<p>This blog explores how I fine tuned Qwen, Qwen/Qwen3-30B-A3B, a chinese open source model producing React code for a fraction of the cost of any of the top-tier AI research lab. I used <a href="http://tinker-docs.thinkingmachines.ai/">Tinker</a> to fine tune my own LLM, a product released by the team at <a href="https://thinkingmachines.ai/">Thinking Machines Lab</a>.</p>
<p>It did a great job, and produced generally usable React code to the level that I expected it to from an online dataset.</p>
<p>In this blog, I explain what fine tuning means, why Generative UI can/should be fine tuned, and left some of my thoughts on tinker.</p>
<h2 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h2>
<p>The intersection of artificial intelligence and user interface development has reached a fascinating inflection point. For decades, building user interfaces required manual coding‚Äîdevelopers translating design specifications into HTML, CSS, and JavaScript line by line. More recently, component libraries and frameworks like React have standardized this process, but the fundamental paradigm remained unchanged: humans write code, computers execute it.</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their application to UI development faces unique challenges. Unlike backend logic or algorithmic problems where correctness is binary, UI code must balance multiple competing objectives: syntactic correctness, visual appeal, interactivity, accessibility, and alignment with user intent.</p>
<p>Enter reinforcement learning. By treating UI generation as a sequential decision-making problem where the model receives rewards for producing high-quality interfaces, we can guide LLMs toward generating not just syntactically correct code, but truly functional, interactive, and well-structured user interfaces. This represents a fundamental shift from imitation learning to reward-driven optimization.</p>
<h2 id="why-generative-ui-and-fine-tuning">Why Generative UI and fine tuning?<a hidden class="anchor" aria-hidden="true" href="#why-generative-ui-and-fine-tuning">#</a></h2>
<p>Why invest effort in fine-tuning models specifically for generative UI? The answer lies in understanding the unique requirements of UI code generation versus general-purpose coding: Structural Completeness, Interactivity and Dynamics, Visual and Semantic Coherence, and Domain-Specific Patterns.</p>
<p>Traditional supervised fine-tuning trains models on input-output pairs, essentially teaching imitation. But what if the training data contains suboptimal examples? What if there are multiple valid solutions with different quality levels? Supervised learning can&rsquo;t distinguish between them‚Äîit treats all training examples as equally correct.</p>
<p>Reinforcement learning, specifically Proximal Policy Optimization (PPO), addresses these limitations by defining explicit reward functions that encode our preferences. We can reward complete code more than truncated outputs, interactive components more than static ones, and idiomatic patterns more than unusual but technically correct alternatives. The model learns through exploration and feedback, discovering better solutions than those in the training data.</p>
<h2 id="definition-of-generative-ui">Definition of Generative UI<a hidden class="anchor" aria-hidden="true" href="#definition-of-generative-ui">#</a></h2>
<p><strong>Generative UI</strong> refers to the automated creation of user interface code through machine learning models, where the system generates complete, functional interface implementations from natural language descriptions or specifications.</p>
<p>More formally, generative UI can be defined as a conditional generation task:</p>
<p>Given:</p>
<ul>
<li>A specification \( S \) (user intent, design requirements, functional specifications)</li>
<li>A domain \( D \) (React, Vue, HTML/CSS, etc.)</li>
<li>A set of constraints \( C \) (style guidelines, accessibility requirements, component libraries)</li>
</ul>
<p>Generate:</p>
<ul>
<li>A complete code artifact \( U \) that:
<ul>
<li>Compiles/parses without errors</li>
<li>Renders a visual interface matching \( S \)</li>
<li>Includes appropriate interactivity and state management</li>
<li>Follows idiomatic patterns for domain \( D \)</li>
<li>Satisfies constraints \( C \)</li>
</ul>
</li>
</ul>
<p>This differs from code completion or snippet generation in crucial ways:</p>
<p><strong>Completeness</strong>: Generative UI produces entire, self-contained components or applications, not fragments requiring human integration.</p>
<p><strong>Functional Correctness</strong>: The generated code must actually work when executed, not just look plausible to static analysis.</p>
<p><strong>Visual Grounding</strong>: The code&rsquo;s visual output matters as much as its textual content‚Äîan interface that compiles but looks broken has failed its purpose.</p>
<p><strong>Context Awareness</strong>: Generative UI systems must understand the broader context of UI development: component composition, state flow, event handling, styling approaches, and accessibility considerations.</p>
<p>In our implementation, we focus on React/TypeScript generation, treating each generation as a mapping from <code>(system_prompt, user_message) ‚Üí React component code</code>. The model must learn the implicit constraints of React development: proper JSX syntax, hook usage rules, event handler patterns, and TypeScript type annotations.</p>
<h2 id="fine-tuning-and-ppo-why-reinforcement-learning-for-generative-ui">Fine-Tuning and PPO: Why Reinforcement Learning for Generative UI<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-and-ppo-why-reinforcement-learning-for-generative-ui">#</a></h2>
<p>Fine-tuning adapts a pre-trained language model to a specific domain or task by continuing training on targeted data. For generative UI, we face a critical question: <em>should we use supervised learning or reinforcement learning?</em></p>
<h3 id="the-supervised-learning-baseline">The Supervised Learning Baseline<a hidden class="anchor" aria-hidden="true" href="#the-supervised-learning-baseline">#</a></h3>
<p>Supervised learning optimizes a straightforward objective‚Äîminimize the cross-entropy loss between predicted and target tokens:</p>
<p>$$\mathcal{L}_{\text{SL}}(\theta) = -\mathbb{E}_{x \sim \mathcal{D}}\left[\sum_{t} \log p_\theta(x_t \mid x_{&lt;t})\right]$$</p>
<p>Where \( \theta \) are model parameters, \( \mathcal{D} \) is the training dataset, \( x_t \) is the token at position \( t \), and \( x_{&lt;t} \) is the context (all previous tokens).</p>
<p>This approach teaches the model to maximize the likelihood of the exact training sequences. It&rsquo;s simple, stable, and works well when training data represents optimal behavior. However, it has fundamental limitations for UI generation:</p>
<ol>
<li><strong>Exposure Bias</strong>: During training, the model always sees correct previous tokens; at inference, its own potentially incorrect predictions compound.</li>
<li><strong>Loss-Evaluation Mismatch</strong>: Cross-entropy loss treats all token prediction errors equally, but in UI code, a mismatched brace is far worse than suboptimal variable naming.</li>
<li><strong>No Exploration</strong>: The model can only learn from provided examples, never discovering alternative (potentially better) solutions.</li>
</ol>
<h3 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)<a hidden class="anchor" aria-hidden="true" href="#proximal-policy-optimization-ppo">#</a></h3>
<p>PPO is a policy gradient reinforcement learning algorithm designed for stable, efficient policy optimization. Instead of maximizing likelihood of specific sequences, PPO maximizes expected reward:</p>
<p>$$\mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[R(\tau)\right]$$</p>
<p>Where:</p>
<ul>
<li>\( \pi_\theta \) is the policy (our language model)</li>
<li>\( \tau \) is a trajectory (generated token sequence)</li>
<li>\( R(\tau) \) is the total reward</li>
</ul>
<p>The core innovation of PPO is the <strong>clipped surrogate objective</strong>, which ensures stable policy updates:</p>
<p>$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$</p>
<p>Where:</p>
<ul>
<li>\( r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} \) is the importance sampling ratio</li>
<li>\( \hat{A}_t \) is the advantage estimate (how much better this action is than average)</li>
<li>\( \epsilon \) is the clipping parameter (typically 0.2)</li>
<li>\( a_t \) and \( s_t \) are the action (token) and state (context) at time \( t \)</li>
</ul>
<p><strong>Why does clipping matter?</strong> Without clipping, the ratio \( r_t(\theta) \) could become arbitrarily large, causing unstable policy updates. If the new policy makes an action much more likely than the old policy (\( r_t \gg 1 \)), we could overshoot the optimal policy. Clipping bounds this ratio to \( [1-\epsilon, 1+\epsilon] \), ensuring conservative updates.</p>
<p>The gradient of this objective is:</p>
<p>$$\nabla_\theta \mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot \min\left(\hat{A}_t, \text{clip}(\hat{A}_t, -\epsilon\hat{A}_t^+, \epsilon\hat{A}_t^-)\right)\right]$$</p>
<p>This gradient increases the probability of actions with positive advantages (good outcomes) and decreases probability of actions with negative advantages (bad outcomes), but only to a limited extent determined by the clipping.</p>
<h3 id="why-ppo-for-generative-ui">Why PPO for Generative UI?<a hidden class="anchor" aria-hidden="true" href="#why-ppo-for-generative-ui">#</a></h3>
<p>PPO addresses the limitations of supervised learning in three critical ways:</p>
<p><strong>1. Reward-Based Optimization</strong><br>
We define a reward function that explicitly encodes UI quality:</p>
<p>$$R(\text{code}) = R_{\text{base}} + w_1 R_{\text{complete}} + w_2 R_{\text{valid}} + w_3 R_{\text{interactive}} + w_4 R_{\text{quotes}} - w_5 \text{penalty}_{\text{length}}$$</p>
<p>Where:</p>
<ul>
<li>\( R_{\text{complete}} \): Reward for structural completeness (matched braces, proper endings)</li>
<li>\( R_{\text{valid}} \): Reward for syntactic validity (balanced delimiters, proper exports)</li>
<li>\( R_{\text{interactive}} \): Reward for interactive features (state hooks, event handlers)</li>
<li>\( R_{\text{quotes}} \): Reward for balanced quotes and strings</li>
<li>\( \text{penalty}_{\text{length}} \): Penalty for excessive length deviation from reference</li>
</ul>
<p>This <strong>multi-objective reward function captures the nuanced requirements of UI code</strong> that cross-entropy loss cannot express.</p>
<p><strong>2. Exploration and Discovery</strong><br>
By sampling multiple completions per prompt (\( k \) samples), the model explores the solution space. Good solutions receive positive advantages, bad solutions receive negative advantages. Over time, the policy shifts toward generating better code:</p>
<p><strong>3. Stability Through Clipping</strong><br>
The clipping mechanism prevents catastrophic policy updates. Even if we sample an unusually good or bad trajectory, the policy update is bounded. This is crucial for fine-tuning pre-trained models where we want to adapt behavior, not destroy existing knowledge.</p>
<h2 id="implementation-asynchronous-ppo-training-with-tinker">Implementation: Asynchronous PPO Training with Tinker<a hidden class="anchor" aria-hidden="true" href="#implementation-asynchronous-ppo-training-with-tinker">#</a></h2>
<p>Implementing PPO for large language models traditionally requires extensive infrastructure: distributed training systems, GPU orchestration, gradient accumulation strategies, and sophisticated async coordination. <strong>Tinker</strong> abstracts this complexity, letting us focus on the algorithm while it handles distributed execution.</p>
<h3 id="algorithm-asynchronous-ppo-training-with-tinker">Algorithm: Asynchronous PPO Training with Tinker<a hidden class="anchor" aria-hidden="true" href="#algorithm-asynchronous-ppo-training-with-tinker">#</a></h3>
<p>My implementation follows the standard PPO training loop with asynchronous sampling and gradient computation:</p>
<p>The key innovation is asynchronous execution. Instead of synchronously sampling one prompt at a time, we launch all sampling requests concurrently, process results as they complete, and overlap computation phases.</p>
<p>Below is an image of the general asynchronous calls that were computed to allow tinker to work efficiently.</p>
<p><img alt="Asynchronous PPO Algorithm" loading="lazy" src="/blog/images/ppo_tinker.png"></p>
<div style="text-align: center; font-style: italic;">Algorithm 1: Asynchronous PPO for Generative UI using Tinker API</div>


<h3 id="reward-function-design">Reward function design<a hidden class="anchor" aria-hidden="true" href="#reward-function-design">#</a></h3>
<p>The reward function is compositional:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_reward</span>(generated_code, reference_code):
</span></span><span style="display:flex;"><span>    R_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Completeness: severely punish truncation</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> is_truncated(generated_code):
</span></span><span style="display:flex;"><span>        R_complete <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">15.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        R_complete <span style="color:#f92672">=</span> <span style="color:#f92672">+</span><span style="color:#ae81ff">7.5</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Validity: reward balanced delimiters</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.8</span> <span style="color:#66d9ef">if</span> balanced_braces(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">3.0</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#66d9ef">if</span> balanced_brackets(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#66d9ef">if</span> balanced_parens(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.2</span> <span style="color:#66d9ef">if</span> has_return_or_export(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Interactivity: reward React patterns</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.25</span> <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;useState&#39;</span> <span style="color:#f92672">in</span> generated_code <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.75</span> <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;useEffect&#39;</span> <span style="color:#f92672">in</span> generated_code <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> count_event_handlers(generated_code) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.25</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#66d9ef">if</span> has_conditional_rendering(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Quotes: reward balanced strings</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.6</span> <span style="color:#66d9ef">if</span> balanced_single_quotes(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.4</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.2</span> <span style="color:#66d9ef">if</span> balanced_double_quotes(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Length penalty: discourage excessive verbosity</span>
</span></span><span style="display:flex;"><span>    penalty_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> abs(len(generated_code) <span style="color:#f92672">-</span> len(reference_code)) <span style="color:#f92672">/</span> len(reference_code)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> R_base <span style="color:#f92672">+</span> R_complete <span style="color:#f92672">+</span> R_valid <span style="color:#f92672">+</span> R_interactive <span style="color:#f92672">+</span> R_quotes <span style="color:#f92672">-</span> penalty_length
</span></span></code></pre></div><p><strong>Advantage Construction</strong><br>
The advantage array is crucial. We assign zero advantage to prompt tokens (we don&rsquo;t train on them) and distribute the trajectory reward across generated tokens:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_advantage_array</span>(prompt_length, gen_length, reward):
</span></span><span style="display:flex;"><span>    prompt_advantages <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.0</span>] <span style="color:#f92672">*</span> (prompt_length <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    gen_advantages <span style="color:#f92672">=</span> [reward] <span style="color:#f92672">*</span> gen_length
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> prompt_advantages <span style="color:#f92672">+</span> gen_advantages
</span></span></code></pre></div><p>This ensures gradients only flow through generated portions from the LLM, not the fixed prompt context.</p>
<p><strong>Hyperparameter Tuning</strong><br>
Our configuration:</p>
<ul>
<li>Learning rate: \( 10^{-5} \) (small to prevent catastrophic forgetting)</li>
<li>Samples per prompt: 4 (exploration-exploitation balance)</li>
<li>Clip epsilon: 0.2 (standard PPO value)</li>
<li>Epochs: 5 (sufficient for convergence on 600 examples)</li>
<li>Max generation tokens: 16,000 (React components can be long)</li>
</ul>
<p><strong>Data Filtering</strong><br>
We filter prompts exceeding 16k tokens during initialization to leave room for generation within the 32k context window. This prevents out-of-memory errors and truncated generations.</p>
<h2 id="results-when-comparing-the-fine-tuned-model-to-raw-qwen-model-samples">Results when comparing the fine-tuned model to raw Qwen model samples<a hidden class="anchor" aria-hidden="true" href="#results-when-comparing-the-fine-tuned-model-to-raw-qwen-model-samples">#</a></h2>
<h2 id="thoughts-on-tinker">Thoughts on Tinker<a hidden class="anchor" aria-hidden="true" href="#thoughts-on-tinker">#</a></h2>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Generative UI represents a paradigm shift in interface development‚Äîfrom manual coding to specification-to-implementation via machine learning. By fine-tuning large language models with Proximal Policy Optimization, we can teach models not just to imitate existing code, but to discover and generate high-quality, interactive, complete user interfaces.</p>
<p>I have focused more on the length of the React code output, and how dynamic it is. Users can target other features such as tailwind/design parity, or code structure.</p>
<p>The combination of PPO&rsquo;s reward-driven learning and Tinker&rsquo;s abstraction of distributed training infrastructure makes this approach practical for researchers and developers. We define our reward function (what makes good UI code), specify our training loop (how to explore and update), and Tinker handles the complexity of distributed execution across GPUs effectively.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/blog/">Pranav on a Tangent</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
