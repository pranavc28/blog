<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Tinkering with Generative UI | Pranav on a Tangent</title>
<meta name="keywords" content="">
<meta name="description" content="&ldquo;Form and function should be one, joined in a spiritual union&rdquo; - Frank Lloyd Wright
Research code: https://github.com/pranavc28/generative-ui
Purpose
This blog explores how I fine tuned Qwen, Qwen/Qwen3-30B-A3B, an open source model producing React code for a fraction of the cost of any of the top-tier AI research lab. I used Tinker to fine tune my own LLM, a product released by the team at Thinking Machines Lab.
It did a great job, and produced generally usable React code to the level that I expected it to from an online dataset.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/blog/posts/generative-ui-tinker/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.svg">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/generative-ui-tinker/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üç∫</text></svg>">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\[', right: '\\]', display: true},
              {left: '\\(', right: '\\)', display: false}
          ],
          throwOnError : false
        });
    });
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Pranav on a Tangent (Alt + H)">Pranav on a Tangent</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Tinkering with Generative UI
    </h1>
    <div class="post-meta"><span title='2025-11-14 07:07:07 +0100 +0100'>November 14, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><em>&ldquo;Form and function should be one, joined in a spiritual union&rdquo; - Frank Lloyd Wright</em></p>
<p>Research code: <a href="https://github.com/pranavc28/generative-ui">https://github.com/pranavc28/generative-ui</a></p>
<h2 id="purpose">Purpose<a hidden class="anchor" aria-hidden="true" href="#purpose">#</a></h2>
<p>This blog explores how I fine tuned Qwen, Qwen/Qwen3-30B-A3B, an open source model producing React code for a fraction of the cost of any of the top-tier AI research lab. I used <a href="http://tinker-docs.thinkingmachines.ai/">Tinker</a> to fine tune my own LLM, a product released by the team at <a href="https://thinkingmachines.ai/">Thinking Machines Lab</a>.</p>
<p>It did a great job, and produced generally usable React code to the level that I expected it to from an online dataset.</p>
<p>In this blog, I explain what fine tuning means, why Generative UI can/should be fine tuned, and left some of my thoughts on tinker.</p>
<h2 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h2>
<p>The intersection of artificial intelligence and user interface development has reached a fascinating inflection point. For decades, building user interfaces required manual coding‚Äîdevelopers translating design specifications into HTML, CSS, and JavaScript line by line. More recently, component libraries and frameworks like React have standardized this process, but the fundamental paradigm remained unchanged: humans write code, computers execute it.</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their application to UI development faces unique challenges. Unlike backend logic or algorithmic problems where correctness is binary, UI code must balance multiple competing objectives: syntactic correctness, visual appeal, interactivity, accessibility, and alignment with user intent.</p>
<p>Enter reinforcement learning. By treating UI generation as a sequential decision-making problem where the model receives rewards for producing high-quality interfaces, we can guide LLMs toward generating not just syntactically correct code, but truly functional, interactive, and well-structured user interfaces. This represents a fundamental shift from imitation learning to reward-driven optimization.</p>
<h2 id="why-generative-ui-and-fine-tuning">Why Generative UI and fine tuning?<a hidden class="anchor" aria-hidden="true" href="#why-generative-ui-and-fine-tuning">#</a></h2>
<p>Why invest effort in fine-tuning models specifically for generative UI? The answer lies in understanding the unique requirements of UI code generation versus general-purpose coding: Structural Completeness, Interactivity and Dynamics, Visual and Semantic Coherence, and Domain-Specific Patterns.</p>
<p>Traditional supervised fine-tuning trains models on input-output pairs, essentially teaching imitation. But what if the training data contains suboptimal examples? What if there are multiple valid solutions with different quality levels? Supervised learning can&rsquo;t distinguish between them‚Äîit treats all training examples as equally correct.</p>
<p>Reinforcement learning, specifically Proximal Policy Optimization (PPO), addresses these limitations by defining explicit reward functions that encode our preferences. We can reward complete code more than truncated outputs, interactive components more than static ones, and idiomatic patterns more than unusual but technically correct alternatives. The model learns through exploration and feedback, discovering better solutions than those in the training data.</p>
<h2 id="definition-of-generative-ui">Definition of Generative UI<a hidden class="anchor" aria-hidden="true" href="#definition-of-generative-ui">#</a></h2>
<p><strong>Generative UI</strong> refers to the automated creation of user interface code through machine learning models, where the system generates complete, functional interface implementations from natural language descriptions or specifications.</p>
<p>More formally, generative UI can be defined as a conditional generation task:</p>
<p>Given:</p>
<ul>
<li>A specification \( S \) (user intent, design requirements, functional specifications)</li>
<li>A domain \( D \) (React, Vue, HTML/CSS, etc.)</li>
<li>A set of constraints \( C \) (style guidelines, accessibility requirements, component libraries)</li>
</ul>
<p>Generate:</p>
<ul>
<li>A complete code artifact \( U \) that:
<ul>
<li>Compiles/parses without errors</li>
<li>Renders a visual interface matching \( S \)</li>
<li>Includes appropriate interactivity and state management</li>
<li>Follows idiomatic patterns for domain \( D \)</li>
<li>Satisfies constraints \( C \)</li>
</ul>
</li>
</ul>
<p>This differs from code completion or snippet generation in crucial ways:</p>
<p><strong>Completeness</strong>: Generative UI produces entire, self-contained components or applications, not fragments requiring human integration.</p>
<p><strong>Functional Correctness</strong>: The generated code must actually work when executed, not just look plausible to static analysis.</p>
<p><strong>Visual Grounding</strong>: The code&rsquo;s visual output matters as much as its textual content‚Äîan interface that compiles but looks broken has failed its purpose.</p>
<p><strong>Context Awareness</strong>: Generative UI systems must understand the broader context of UI development: component composition, state flow, event handling, styling approaches, and accessibility considerations.</p>
<p>In our implementation, we focus on React/TypeScript generation, treating each generation as a mapping from <code>(system_prompt, user_message) ‚Üí React component code</code>. The model must learn the implicit constraints of React development: proper JSX syntax, hook usage rules, event handler patterns, and TypeScript type annotations.</p>
<h2 id="fine-tuning-and-ppo-why-reinforcement-learning-for-generative-ui">Fine-Tuning and PPO: Why Reinforcement Learning for Generative UI<a hidden class="anchor" aria-hidden="true" href="#fine-tuning-and-ppo-why-reinforcement-learning-for-generative-ui">#</a></h2>
<p>Fine-tuning adapts a pre-trained language model to a specific domain or task by continuing training on targeted data. For generative UI, we face a critical question: <em>should we use supervised learning or reinforcement learning?</em></p>
<h3 id="the-supervised-learning-baseline">The Supervised Learning Baseline<a hidden class="anchor" aria-hidden="true" href="#the-supervised-learning-baseline">#</a></h3>
<p>Supervised learning optimizes a straightforward objective‚Äîminimize the cross-entropy loss between predicted and target tokens:</p>
<p>$$\mathcal{L}_{\text{SL}}(\theta) = -\mathbb{E}_{x \sim \mathcal{D}}\left[\sum_{t} \log p_\theta(x_t \mid x_{&lt;t})\right]$$</p>
<p>Where \( \theta \) are model parameters, \( \mathcal{D} \) is the training dataset, \( x_t \) is the token at position \( t \), and \( x_{&lt;t} \) is the context (all previous tokens).</p>
<p>This approach teaches the model to maximize the likelihood of the exact training sequences. It&rsquo;s simple, stable, and works well when training data represents optimal behavior. However, it has fundamental limitations for UI generation:</p>
<ol>
<li><strong>Exposure Bias</strong>: During training, the model always sees correct previous tokens; at inference, its own potentially incorrect predictions compound.</li>
<li><strong>Loss-Evaluation Mismatch</strong>: Cross-entropy loss treats all token prediction errors equally, but in UI code, a mismatched brace is far worse than suboptimal variable naming.</li>
<li><strong>No Exploration</strong>: The model can only learn from provided examples, never discovering alternative (potentially better) solutions.</li>
</ol>
<h3 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)<a hidden class="anchor" aria-hidden="true" href="#proximal-policy-optimization-ppo">#</a></h3>
<p>PPO is a policy gradient reinforcement learning algorithm designed for stable, efficient policy optimization. Instead of maximizing likelihood of specific sequences, PPO maximizes expected reward:</p>
<p>$$\mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[R(\tau)\right]$$</p>
<p>Where:</p>
<ul>
<li>\( \pi_\theta \) is the policy (our language model)</li>
<li>\( \tau \) is a trajectory (generated token sequence)</li>
<li>\( R(\tau) \) is the total reward</li>
</ul>
<p>The core innovation of PPO is the <strong>clipped surrogate objective</strong>, which ensures stable policy updates:</p>
<p>$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$</p>
<p>Where:</p>
<ul>
<li>\( r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} \) is the importance sampling ratio</li>
<li>\( \hat{A}_t \) is the advantage estimate (how much better this action is than average)</li>
<li>\( \epsilon \) is the clipping parameter (typically 0.2)</li>
<li>\( a_t \) and \( s_t \) are the action (token) and state (context) at time \( t \)</li>
</ul>
<p><strong>Why does clipping matter?</strong> Without clipping, the ratio \( r_t(\theta) \) could become arbitrarily large, causing unstable policy updates. If the new policy makes an action much more likely than the old policy (\( r_t \gg 1 \)), we could overshoot the optimal policy. Clipping bounds this ratio to \( [1-\epsilon, 1+\epsilon] \), ensuring conservative updates.</p>
<p>The gradient of this objective is:</p>
<p>$$\nabla_\theta \mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t) \cdot \min\left(\hat{A}_t, \text{clip}(\hat{A}_t, -\epsilon\hat{A}_t^+, \epsilon\hat{A}_t^-)\right)\right]$$</p>
<p>This gradient increases the probability of actions with positive advantages (good outcomes) and decreases probability of actions with negative advantages (bad outcomes), but only to a limited extent determined by the clipping.</p>
<h3 id="why-ppo-for-generative-ui">Why PPO for Generative UI?<a hidden class="anchor" aria-hidden="true" href="#why-ppo-for-generative-ui">#</a></h3>
<p>PPO addresses the limitations of supervised learning in three critical ways:</p>
<p><strong>1. Reward-Based Optimization</strong><br>
We define a reward function that explicitly encodes UI quality:</p>
<p>$$R(\text{code}) = R_{\text{base}} + w_1 R_{\text{complete}} + w_2 R_{\text{valid}} + w_3 R_{\text{interactive}} + w_4 R_{\text{quotes}} - w_5 \text{penalty}_{\text{length}}$$</p>
<p>Where:</p>
<ul>
<li>\( R_{\text{complete}} \): Reward for structural completeness (matched braces, proper endings)</li>
<li>\( R_{\text{valid}} \): Reward for syntactic validity (balanced delimiters, proper exports)</li>
<li>\( R_{\text{interactive}} \): Reward for interactive features (state hooks, event handlers)</li>
<li>\( R_{\text{quotes}} \): Reward for balanced quotes and strings</li>
<li>\( \text{penalty}_{\text{length}} \): Penalty for excessive length deviation from reference</li>
</ul>
<p>This <strong>multi-objective reward function captures the nuanced requirements of UI code</strong> that cross-entropy loss cannot express.</p>
<p><strong>2. Exploration and Discovery</strong><br>
By sampling multiple completions per prompt (\( k \) samples), the model explores the solution space. Good solutions receive positive advantages, bad solutions receive negative advantages. Over time, the policy shifts toward generating better code:</p>
<p><strong>3. Stability Through Clipping</strong><br>
The clipping mechanism prevents catastrophic policy updates. Even if we sample an unusually good or bad trajectory, the policy update is bounded. This is crucial for fine-tuning pre-trained models where we want to adapt behavior, not destroy existing knowledge.</p>
<h2 id="implementation-asynchronous-ppo-training-with-tinker">Implementation: Asynchronous PPO Training with Tinker<a hidden class="anchor" aria-hidden="true" href="#implementation-asynchronous-ppo-training-with-tinker">#</a></h2>
<p>Implementing PPO for large language models traditionally requires extensive infrastructure: distributed training systems, GPU orchestration, gradient accumulation strategies, and sophisticated async coordination. <strong>Tinker</strong> abstracts this complexity, letting us focus on the algorithm while it handles distributed execution.</p>
<h3 id="algorithm-asynchronous-ppo-training-with-tinker">Algorithm: Asynchronous PPO Training with Tinker<a hidden class="anchor" aria-hidden="true" href="#algorithm-asynchronous-ppo-training-with-tinker">#</a></h3>
<p>My implementation follows the standard PPO training loop with asynchronous sampling and gradient computation:</p>
<p>The key innovation is asynchronous execution. Instead of synchronously sampling one prompt at a time, we launch all sampling requests concurrently, process results as they complete, and overlap computation phases.</p>
<p>Below is an image of the general asynchronous calls that were computed to allow tinker to work efficiently.</p>
<p><img alt="Asynchronous PPO Algorithm" loading="lazy" src="/blog/images/ppo_tinker.png"></p>
<div style="text-align: center; font-style: italic;">Algorithm 1: Asynchronous PPO for Generative UI using Tinker API</div>


<h3 id="reward-function-design">Reward function design<a hidden class="anchor" aria-hidden="true" href="#reward-function-design">#</a></h3>
<p>The reward function is compositional:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_reward</span>(generated_code, reference_code):
</span></span><span style="display:flex;"><span>    R_base <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Completeness: severely punish truncation</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> is_truncated(generated_code):
</span></span><span style="display:flex;"><span>        R_complete <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">15.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        R_complete <span style="color:#f92672">=</span> <span style="color:#f92672">+</span><span style="color:#ae81ff">7.5</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Validity: reward balanced delimiters</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.8</span> <span style="color:#66d9ef">if</span> balanced_braces(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">3.0</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#66d9ef">if</span> balanced_brackets(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.9</span> <span style="color:#66d9ef">if</span> balanced_parens(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.5</span>
</span></span><span style="display:flex;"><span>    R_valid <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.2</span> <span style="color:#66d9ef">if</span> has_return_or_export(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Interactivity: reward React patterns</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.25</span> <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;useState&#39;</span> <span style="color:#f92672">in</span> generated_code <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.75</span> <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;useEffect&#39;</span> <span style="color:#f92672">in</span> generated_code <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> count_event_handlers(generated_code) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.25</span>
</span></span><span style="display:flex;"><span>    R_interactive <span style="color:#f92672">+=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#66d9ef">if</span> has_conditional_rendering(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Quotes: reward balanced strings</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.6</span> <span style="color:#66d9ef">if</span> balanced_single_quotes(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.4</span>
</span></span><span style="display:flex;"><span>    R_quotes <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.2</span> <span style="color:#66d9ef">if</span> balanced_double_quotes(generated_code) <span style="color:#66d9ef">else</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">2.0</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Length penalty: discourage excessive verbosity</span>
</span></span><span style="display:flex;"><span>    penalty_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> abs(len(generated_code) <span style="color:#f92672">-</span> len(reference_code)) <span style="color:#f92672">/</span> len(reference_code)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> R_base <span style="color:#f92672">+</span> R_complete <span style="color:#f92672">+</span> R_valid <span style="color:#f92672">+</span> R_interactive <span style="color:#f92672">+</span> R_quotes <span style="color:#f92672">-</span> penalty_length
</span></span></code></pre></div><p><strong>Advantage Construction</strong><br>
The advantage array is crucial. We assign zero advantage to prompt tokens (we don&rsquo;t train on them) and distribute the trajectory reward across generated tokens:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_advantage_array</span>(prompt_length, gen_length, reward):
</span></span><span style="display:flex;"><span>    prompt_advantages <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.0</span>] <span style="color:#f92672">*</span> (prompt_length <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    gen_advantages <span style="color:#f92672">=</span> [reward] <span style="color:#f92672">*</span> gen_length
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> prompt_advantages <span style="color:#f92672">+</span> gen_advantages
</span></span></code></pre></div><p>This ensures gradients only flow through generated portions from the LLM, not the fixed prompt context.</p>
<p><strong>Hyperparameter Tuning</strong><br>
Our configuration:</p>
<ul>
<li>Learning rate: \( 10^{-5} \) (small to prevent catastrophic forgetting)</li>
<li>Samples per prompt: 4 (exploration-exploitation balance)</li>
<li>Clip epsilon: 0.2 (standard PPO value)</li>
<li>Epochs: 5 (sufficient for convergence on 600 examples)</li>
<li>Max generation tokens: 16,000 (React components can be long)</li>
</ul>
<p><strong>Data Filtering</strong><br>
We filter prompts exceeding 16k tokens during initialization to leave room for generation within the 32k context window. This prevents out-of-memory errors and truncated generations.</p>
<h2 id="results-when-comparing-the-fine-tuned-model-to-raw-qwen-model-samples">Results when comparing the fine-tuned model to raw Qwen model samples<a hidden class="anchor" aria-hidden="true" href="#results-when-comparing-the-fine-tuned-model-to-raw-qwen-model-samples">#</a></h2>
<p>To evaluate the effectiveness of PPO fine-tuning, I tested both the raw Qwen model and the fine-tuned version on three distinct UI generation tasks: booking a ride interface, a Google search homepage, and a leaderboard display. The differences are striking and directly reflect the reward signals defined in our training objective.</p>
<h3 id="use-case-1-ride-booking-interface">Use Case 1: Ride Booking Interface<a hidden class="anchor" aria-hidden="true" href="#use-case-1-ride-booking-interface">#</a></h3>
<p><strong>Raw Model Performance</strong></p>
<p><img alt="Raw Qwen Transport Interface Failure" loading="lazy" src="/blog/images/sample_qwen_transport_fail.png"></p>
<p>The raw model&rsquo;s output exhibits the most critical failure: <strong>Incomplete code generation</strong>: the component is truncated mid-function, missing closing braces and return statements.</p>
<p><strong>Fine-Tuned Model Performance</strong></p>
<p><img alt="Fine-Tuned Qwen Transport Interface" loading="lazy" src="/blog/images/fine_tune_qwen_easy_rider_transport.png"></p>
<p><img alt="Fine-Tuned Live Ride Booking" loading="lazy" src="/blog/images/fine_tune_ride_book_live.png"></p>
<p>The fine-tuned model demonstrates substantial improvement:</p>
<ul>
<li><strong>Structural completeness</strong>: Fully formed component with proper exports and matched delimiters</li>
<li><strong>Rich interactivity</strong>: Multiple <code>useState</code> hooks managing ride selection, pickup/dropoff locations, and booking confirmation</li>
<li><strong>Event handler coverage</strong>: onClick handlers for ride type selection, onChange for input fields, onSubmit for booking confirmation</li>
<li><strong>Conditional rendering</strong>: Dynamic UI showing different states (selecting ride, confirming booking, success message)</li>
</ul>
<p>The fine-tuned model generates not just syntactically correct code, but a <strong>functionally complete, interactive booking flow</strong> that responds to user actions‚Äîexactly what our reward function incentivized. Given the addition of a new computational reward variable for dynamic code, we can see that the LLM picks up on this use case for code generation.</p>
<h3 id="use-case-2-google-search-homepage">Use Case 2: Google Search Homepage<a hidden class="anchor" aria-hidden="true" href="#use-case-2-google-search-homepage">#</a></h3>
<p><strong>Raw Model Performance</strong></p>
<p><img alt="Raw Qwen Google Homepage" loading="lazy" src="/blog/images/qwen_sample_google_homepage.png"></p>
<p>The raw model produces a basic structure but falls short:</p>
<ul>
<li><strong>Limited interactivity</strong>: Search input exists but lacks proper state management</li>
<li><strong>Missing event handlers</strong>: No onSubmit handler for search functionality, no onChange for input updates</li>
<li><strong>Static elements</strong>: Buttons and links that don&rsquo;t respond to user interaction</li>
<li><strong>Truncation issues</strong>: May cut off mid-styling or mid-component definition</li>
</ul>
<p><strong>Fine-Tuned Model Performance</strong></p>
<p><img alt="Fine-Tuned Google Search Homepage" loading="lazy" src="/blog/images/fine_tune_google_search_works.png"></p>
<p>The fine-tuned model excels:</p>
<ul>
<li><strong>Complete state management</strong>: <code>useState</code> hook managing search query input</li>
<li><strong>Functional search</strong>: Proper form submission with <code>onSubmit</code> handler, and clear search query processing even if the results are hardcoded</li>
<li><strong>Reactive UI</strong>: Input field updates in real-time with <code>onChange</code> handler, controlled component pattern</li>
<li><strong>Visual polish</strong>: Complete styling that matches the Google aesthetic, properly balanced quotes in className strings</li>
<li><strong>Balanced delimiters</strong>: All braces, brackets, and parentheses properly matched</li>
</ul>
<p>The reward function&rsquo;s emphasis on interactivity (\( R_{\text{interactive}} \)) is clearly visible‚Äîthe fine-tuned model doesn&rsquo;t just render a static homepage mockup, it generates a <strong>functional search interface with proper React patterns</strong>.</p>
<h3 id="use-case-3-leaderboard-display">Use Case 3: Leaderboard Display<a hidden class="anchor" aria-hidden="true" href="#use-case-3-leaderboard-display">#</a></h3>
<p><strong>Raw Model Performance</strong></p>
<p><img alt="Raw Qwen Leaderboard" loading="lazy" src="/blog/images/raw_leaderboard.png"></p>
<p>The raw model&rsquo;s leaderboard suffers from:</p>
<ul>
<li><strong>Incomplete rendering</strong>: May truncate the list of players or miss closing tags</li>
<li><strong>Static data</strong>: Hardcoded player list with no dynamic updates or sorting</li>
<li><strong>Poor data structure</strong>: Inconsistent formatting or missing key player attributes</li>
<li><strong>No interactivity</strong>: Unable to filter, sort, or update leaderboard dynamically</li>
</ul>
<p><strong>Fine-Tuned Model Performance</strong> <em>(inferred from training patterns)</em></p>
<p>The fine-tuned model generates improved leaderboards with:</p>
<ul>
<li><strong>Complete table structure</strong>: All rows and columns properly closed, balanced JSX elements</li>
<li><strong>State-driven rendering</strong>: Uses <code>useState</code> to manage leaderboard data, enabling dynamic updates. Also updates leaderboards in realtime as a demo as time progresses.</li>
<li><strong>Interactive features</strong>: Sorting by clicking column headers, filtering by player name, expandable rows for player details</li>
<li><strong>Conditional rendering</strong>: Highlights for top players, empty state handling when no data exists</li>
<li><strong>Proper data mapping</strong>: <code>.map()</code> with keys, proper TypeScript typing for player objects</li>
</ul>
<p>In this case, I was super interested with the model picking up real time updates using time. This was not something that I explicitly prompted the LLM. Yet, it reasoned from previous examples that real time updates could be valuable.</p>
<h2 id="thoughts-on-tinker">Thoughts on Tinker<a hidden class="anchor" aria-hidden="true" href="#thoughts-on-tinker">#</a></h2>
<p>My experience using Tinker for this generative UI fine-tuning project revealed several strengths and limitations worth discussing.</p>
<p><strong>Pros:</strong></p>
<ol>
<li>
<p><strong>Flexible API for Custom Training Loops</strong>: Tinker&rsquo;s Python-based API allowed me to implement asynchronous PPO with full control over the training loop. I could define custom reward functions, manage sampling strategies, and orchestrate advantage computations without being constrained by a rigid framework. This flexibility was essential for implementing the multi-objective reward function targeting UI-specific qualities like completeness, interactivity, and structural validity. The ability to write arbitrary Python code while Tinker handled the distributed execution was the key differentiator.</p>
</li>
<li>
<p><strong>Managed Infrastructure with Distributed GPU Orchestration</strong>: The platform abstracted away the complexity of distributed training across multiple GPUs. I didn&rsquo;t need to manage NCCL configurations, handle gradient synchronization, or debug multi-node communication failures. Tinker&rsquo;s infrastructure automatically distributed my sampling requests across available GPUs, collected results asynchronously, and executed gradient updates efficiently. For a researcher focused on algorithm development rather than DevOps, this was invaluable‚ÄîI could iterate on reward functions and training hyperparameters without worrying about infrastructure scalability.</p>
</li>
<li>
<p><strong>Efficient LoRA Support for Large Model Fine-Tuning</strong>: Training a 30B parameter model like Qwen3-30B-A3B would typically require prohibitive computational resources. Tinker&rsquo;s native support for Low-Rank Adaptation (LoRA) reduced the trainable parameters by several orders of magnitude, making the fine-tuning feasible on available GPU resources. The training converged in 5 epochs across 600 examples, completing in a reasonable timeframe. Without LoRA support integrated into the platform, this project would have been impractical for individual researchers or small teams.</p>
</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>
<p><strong>Limited Access and Onboarding Friction</strong>: Tinker is currently in private beta with waitlist-controlled access. While I was fortunate to gain access, along with other major reserach labs such as Ramp, Meta, etc.. the restricted availability could be a significant barrier for teams needing immediate deployment or researchers wanting to replicate my results. The onboarding process required coordination with the Thinking Machines team on Slack, which added lead time before I could begin experimentation. For production use cases with tight timelines, this gating mechanism could be time consuming.</p>
</li>
<li>
<p><strong>Learning Curve for API-Driven Workflows</strong>: I intially built out a synchronous pipeline. It took me a week to figure out that Tinker also natively supports asynchrnous sampling and workflows. The real question that I had was why use synchronous workflows from PPO and LoRa fine tuning? Almost everyone cares about speed - so why not just default all of our workflows to asynchronous.</p>
</li>
<li>
<p><strong>Debugging</strong>: I did not see any support for monitoring or reliability. I wanted to log my epoch runs, samples, and overall fune tuning times. This was not natively supported, and required custom code. I assume that most startups/customers would need better logging or reliability which could be built inherently.</p>
</li>
<li>
<p><strong>Moat</strong>: Oother startups like Modal can also fine tune LLMs efficiently: <a href="https://modal.com/blog/llm-fine-tuning-overview#where-to-fine-tune-llms-in-2025">https://modal.com/blog/llm-fine-tuning-overview#where-to-fine-tune-llms-in-2025</a>. I think what would have been even more impactful is setting up a reward function for me, given a text prompt. Turning the knobs for each particular reward is the real hard part. If a startup did that for me, and did it well with experimentation, they could find a niche in the market that no one has solved yet.</p>
</li>
</ol>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Generative UI represents a paradigm shift in interface development‚Äîfrom manual coding to specification-to-implementation via machine learning. By fine-tuning large language models with Proximal Policy Optimization, we can teach models not just to imitate existing code, but to discover and generate high-quality, interactive, complete user interfaces.</p>
<p>I have focused more on the length of the React code output, and how dynamic it is. Users can target other features such as tailwind/design parity, or code structure.</p>
<p>The combination of PPO&rsquo;s reward-driven learning and Tinker&rsquo;s abstraction of distributed training infrastructure makes this approach practical for researchers and developers. We define our reward function (what makes good UI code), specify our training loop (how to explore and update), and Tinker handles the complexity of distributed execution across GPUs effectively.</p>
<p>I appreciated testing out open source chinese models, like Qwen. Honestly, <strong>I&rsquo;m in shock</strong>. They&rsquo;re worth a fraction of the cost of models like GPT-5 and Gemini-3. In addition, after being fine tuned, my custom checkpoint did an absolutely amazing job. If anything, this blog post got me even more excited about the possibilities and future of AI. If I can fine tune LLMs in my free time so easily, with free credits, imagine what a startup with 100s of millions of dollars in funding must be doing :)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/blog/">Pranav on a Tangent</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
