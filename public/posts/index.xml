<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Pranav on a Tangent</title>
    <link>http://localhost:57054/blog/posts/</link>
    <description>Recent content in Posts on Pranav on a Tangent</description>
    <generator>Hugo -- 0.150.1</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 07:07:07 +0100</lastBuildDate>
    <atom:link href="http://localhost:57054/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tinkering with Generative UI</title>
      <link>http://localhost:57054/blog/posts/generative-ui-tinker/</link>
      <pubDate>Fri, 14 Nov 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:57054/blog/posts/generative-ui-tinker/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;The intersection of artificial intelligence and user interface development has reached a fascinating inflection point. For decades, building user interfaces required manual coding—developers translating design specifications into HTML, CSS, and JavaScript line by line. More recently, component libraries and frameworks like React have standardized this process, but the fundamental paradigm remained unchanged: humans write code, computers execute it.&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, but their application to UI development faces unique challenges. Unlike backend logic or algorithmic problems where correctness is binary, UI code must balance multiple competing objectives: syntactic correctness, visual appeal, interactivity, accessibility, and alignment with user intent. Traditional supervised learning approaches train models to mimic existing code examples, but this paradigm has limitations—it can only teach models to replicate patterns they&amp;rsquo;ve seen before, not to explore and discover better solutions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Thinking to Knowing: Using Natural Language Confidence From LLM Thought Processes</title>
      <link>http://localhost:57054/blog/posts/thought-engineering-self-awareness/</link>
      <pubDate>Sun, 19 Oct 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:57054/blog/posts/thought-engineering-self-awareness/</guid>
      <description>&lt;p&gt;&amp;ldquo;And this is wisdom and temperance and self-knowledge — for a man to know what he knows, and what he does not know.&amp;rdquo; - Plato, Charmides&lt;/p&gt;
&lt;p&gt;&amp;ldquo;To say you know when you know, and to say you do not when you do not — that is knowledge.&amp;rdquo; - Confucius&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&#34;https://www.yash-sharma.com/&#34;&gt;Yash Sharma&lt;/a&gt; for a lot of valuable feedback on my idea and evaluation methodology&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Research code: &lt;a href=&#34;https://github.com/pranavc28/thought-engineering-and-self-awareness&#34;&gt;https://github.com/pranavc28/thought-engineering-and-self-awareness&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;claim&#34;&gt;Claim&lt;/h1&gt;
&lt;p&gt;Thought engineering techniques (overthinking and automated confidence refinement) improve multi-classification performance across all model architectures. The purpose of this blog is to explain these terms, and prove why this is true.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Temperature Sampling for OCR-VQA: Does It Matter?</title>
      <link>http://localhost:57054/blog/posts/temperature-ocr-vqa/</link>
      <pubDate>Sat, 27 Sep 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:57054/blog/posts/temperature-ocr-vqa/</guid>
      <description>&lt;p&gt;Research code: &lt;a href=&#34;https://github.com/pranavc28/temperature-ocr-vqa&#34;&gt;https://github.com/pranavc28/temperature-ocr-vqa&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Temperature in LLMs&lt;/strong&gt; controls how predictable or exploratory a model’s outputs are. Low temperature = consistent and factual, good for precise tasks. High temperature = more diverse, good for creative tasks—but also riskier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Question Answering (VQA)&lt;/strong&gt; is about answering questions directly from images. For OCR tasks, like reading a book cover, VQA can outperform raw OCR because it focuses only on what’s asked (e.g., &lt;em&gt;“Who’s the author?”&lt;/em&gt;) instead of dumping every piece of text.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
