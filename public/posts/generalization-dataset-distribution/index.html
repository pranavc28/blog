<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Dataset Distribution for Generalization: a case study in self-awareness | Pranav on a Tangent</title>
<meta name="keywords" content="">
<meta name="description" content="&ldquo;In all things the middle state is to be praised.&rdquo; - Aristotle
Research code: https://github.com/pranavc28/self-awareness-grpo
LLM: openai/gpt-oss-20b
Datasets: https://github.com/tdiggelm/climate-fever-dataset, https://fever.ai/download/fever/train.jsonl, https://github.com/TalSchuster/VitaminC
Claim
Diverse training datasets improve generalization, with harder examples requiring stronger representation than easier ones to effectively shift model behavior. However, training exclusively on difficult examples fails to transfer to simpler domains‚Äîa mixture of both easy and hard examples is necessary for robust cross-domain performance.
The domain considered is self-awareness. Follow my previous blogs for motivation and background.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/blog/posts/generalization-dataset-distribution/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.svg">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/posts/generalization-dataset-distribution/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üç∫</text></svg>">


<style>
.first-entry {
    min-height: auto;
    margin: var(--gap) 0;
}
.first-entry.home-info {
    padding: 20px 0;
}
</style>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\[', right: '\\]', display: true},
              {left: '\\(', right: '\\)', display: false}
          ],
          throwOnError : false
        });
    });
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/blog/" accesskey="h" title="Pranav on a Tangent (Alt + H)">Pranav on a Tangent</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Dataset Distribution for Generalization: a case study in self-awareness
    </h1>
    <div class="post-meta"><span title='2026-01-20 07:07:07 +0100 +0100'>January 20, 2026</span>

</div>
  </header> 
  <div class="post-content"><p><em>&ldquo;In all things the middle state is to be praised.&rdquo; - <strong>Aristotle</strong></em></p>
<p><strong>Research code:</strong> <a href="https://github.com/pranavc28/self-awareness-grpo">https://github.com/pranavc28/self-awareness-grpo</a></p>
<p><strong>LLM:</strong> openai/gpt-oss-20b</p>
<p><strong>Datasets:</strong> <a href="https://github.com/tdiggelm/climate-fever-dataset">https://github.com/tdiggelm/climate-fever-dataset</a>, <a href="https://fever.ai/download/fever/train.jsonl">https://fever.ai/download/fever/train.jsonl</a>, <a href="https://github.com/TalSchuster/VitaminC">https://github.com/TalSchuster/VitaminC</a></p>
<h2 id="claim">Claim<a hidden class="anchor" aria-hidden="true" href="#claim">#</a></h2>
<p>Diverse training datasets improve generalization, with harder examples requiring stronger representation than easier ones to effectively shift model behavior. However, training exclusively on difficult examples fails to transfer to simpler domains‚Äîa mixture of both easy and hard examples is necessary for robust cross-domain performance.</p>
<p>The domain considered is self-awareness. Follow my previous blogs for motivation and background.</p>
<h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>Generealization is starting to become a more spoken buzzword in ML and AI podcasts. I wanted to give it a stab, and try to formulate my own theories especially in the area of self-awareness.</p>
<h2 id="literature-review">Literature Review<a hidden class="anchor" aria-hidden="true" href="#literature-review">#</a></h2>
<p>I situate this work within the study of self-awareness in LLMs‚Äîthe capacity to recognize knowledge boundaries and calibrate outputs accordingly. Prior research shows LLMs are frequently overconfident rather than acknowledging uncertainty [1], while the VitaminC benchmark demonstrated that contrastive evidence training improves calibration by forcing attention to subtle factual distinctions [2]. I extend this by framing self-awareness as trainable: through GRPO, I incentivize correct &ldquo;Not Enough Info&rdquo; predictions when evidence is genuinely insufficient, directly addressing the &ldquo;NA collapse&rdquo; phenomenon where models exploit the neutral class to avoid penalty.</p>
<p>My claim that diverse training distributions are essential for self-aware generalization builds on hard example mining literature while identifying a critical limitation: training exclusively on difficult datasets fails to transfer to simpler domains. Research confirms hard examples shift model behavior toward nuanced reasoning [3], yet over-specialization degrades foundational performance [4]. I employ GRPO‚Äîcomputing advantages relative to sampled outputs without a separate critic [5]‚Äîto reward calibrated predictions across the full difficulty spectrum, ensuring the model develops genuine self-awareness rather than domain-specific shortcuts.</p>
<h2 id="methodology">Methodology<a hidden class="anchor" aria-hidden="true" href="#methodology">#</a></h2>
<p>I framed this research problem as a fact verification task for self-awareness for each trained model: classifying claims as SUPPORTED (PASS), REFUTED (FAIL), or NOT ENOUGH INFO (NA) given evidence. I constructed mixed training datasets from three benchmarks in different dostributions ‚Äî FEVER, VitaminC, and ClimateFEVER ‚Äî to ensure cross-domain generalization.</p>
<p>The training set in total was balanced with approximately 460 NA, 460 PASS, and 380 FAIL examples. I used different random seeds for training (42) and evaluation (43) to ensure strict train-test separation. Each claim was paired with retrieved evidence text from Wikipedia or the respective dataset source.</p>
<p>To investigate the role of dataset composition in model generalization, I conducted training runs across three distinct configurations:</p>
<ol>
<li>
<p><strong>Hard-only configuration:</strong> Training exclusively on ClimateFEVER and VitaminC‚Äîthe two datasets where the base model exhibits weaker performance‚Äîexcluding the easier FEVER dataset entirely.</p>
</li>
<li>
<p><strong>Hard-weighted configuration:</strong> Training on all three datasets but with distributions favoring the harder ClimateFEVER and VitaminC examples, while including a smaller proportion of FEVER examples.</p>
</li>
<li>
<p><strong>Balanced configuration:</strong> Training with equal representation from all three datasets, giving each source comparable weight in the training mixture.
This experimental design enabled direct comparison of how training data diversity and difficulty distribution affect downstream performance across both challenging and straightforward fact verification domains.</p>
</li>
</ol>
<p>I implemented Group Relative Policy Optimization (GRPO) with LoRA fine-tuning (rank 32) on a GPT-OSS-20B base model. The reward function combined four components: class-weighted accuracy rewards favoring PASS/FAIL predictions, a false-NA penalty, an exploration bonus to prevent NA collapse, and a format compliance penalty given that the model is pretrained and has not undergone supervised fine tuning for instruction following. Training ran for 200 steps with batch size 32, group size 8, and learning rate 5e-6 with AdamW optimization. I set sampling temperature to 0.3 to balance exploration with training stability, given that over this temperatre the formatting penalty would take longer to converge to 0.</p>
<p>For the experimental comparison, I evaluated two checkpoints: a format-only baseline (learned output structure but not accuracy) and the full GRPO model with accuracy optimization. Both models received identical prompts and classified the same 1,000 evaluation examples per dataset (500 NA, 250 PASS, 250 FAIL). I used the exact same prompt template during evaluation as during training to ensure fair comparison. <strong>This paired design allows direct comparison of predictions on identical test instances.</strong></p>
<p>I assessed statistical significance using paired bootstrap testing (10,000 iterations) across accuracy, macro F1, and per-class F1 scores with a threshold of p &lt; 0.01. I computed 95% confidence intervals for all metric differences and employed McNemar&rsquo;s test to quantify discordant predictions. This approach ensures observed improvements reflect genuine capability differences rather than sampling variance.</p>
<h2 id="reward-function-deep-dive-in-grpo">Reward Function Deep Dive in GRPO<a hidden class="anchor" aria-hidden="true" href="#reward-function-deep-dive-in-grpo">#</a></h2>
<p>The total reward for each sampled completion is:</p>
\[
R = R_{\text{correct}} + R_{\text{false\_na}} + R_{\text{explore}} + R_{\text{fmt}}
\]<p><strong>Component Definitions:</strong></p>
\[
R_{\text{correct}} = \begin{cases} +4.0 \times w_{\hat{y}} & \text{if } \hat{y} = y \\ -0.5 \times w_{\hat{y}} & \text{if } \hat{y} \neq y \end{cases}
\]\[
R_{\text{false\_na}} = \begin{cases} -0.5 \times w_{\text{NA}} & \text{if } \hat{y} = \text{NA} \land y \neq \text{NA} \\ 0 & \text{otherwise} \end{cases}
\]\[
R_{\text{explore}} = \begin{cases} +8.0 & \text{if } \hat{y} \in \{\text{PASS}, \text{FAIL}\} \\ 0 & \text{if } \hat{y} = \text{NA} \end{cases}
\]\[
R_{\text{fmt}} = \begin{cases} -0.25 & \text{if output format invalid} \\ 0 & \text{otherwise} \end{cases}
\]<hr>
<p><strong>Where:</strong></p>
<ul>
<li>\(\hat{y}\) is the predicted label (PASS, FAIL, or NA)</li>
<li>\(y\) is the ground truth label</li>
<li>\(w_{\hat{y}}\) is the class weight for the predicted label</li>
</ul>
<hr>
<p><strong>Class Weights:</strong></p>
<table>
  <thead>
      <tr>
          <th>Class</th>
          <th>Weight</th>
          <th>Rationale</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>\(w_{\text{NA}}\)</td>
          <td>0.15</td>
          <td>Low weight discourages defaulting to &ldquo;safe&rdquo; NA</td>
      </tr>
      <tr>
          <td>\(w_{\text{PASS}}\)</td>
          <td>2.5</td>
          <td>Higher weight‚ÄîPASS requires confident assertion</td>
      </tr>
      <tr>
          <td>\(w_{\text{FAIL}}\)</td>
          <td>3.5</td>
          <td>Highest weight‚ÄîFAIL is hardest to predict correctly</td>
      </tr>
  </tbody>
</table>
<hr>
<p><strong>Component Explanations:</strong></p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>\(R_{\text{correct}}\)</td>
          <td>Accuracy signal‚Äîrewards correct predictions, penalizes incorrect ones</td>
      </tr>
      <tr>
          <td>\(R_{\text{false\_na}}\)</td>
          <td>Penalizes NA predictions when evidence clearly exists</td>
      </tr>
      <tr>
          <td>\(R_{\text{explore}}\)</td>
          <td>Rewards PASS/FAIL attempts to encourage exploration</td>
      </tr>
      <tr>
          <td>\(R_{\text{fmt}}\)</td>
          <td>Penalizes invalid output format</td>
      </tr>
  </tbody>
</table>
<hr>
<p><strong>Design Rationale.</strong> The asymmetric class weights (NA=0.15, PASS=2.5, FAIL=3.5) combat NA collapse‚Äîthe model&rsquo;s tendency to default to the &ldquo;safe&rdquo; NA prediction. The large exploration bonus (+8.0) makes attempting PASS/FAIL predictions immediately rewarding, while the gentle incorrect penalty (0.5) reduces risk aversion during exploration.</p>
<h2 id="training-curves">Training Curves<a hidden class="anchor" aria-hidden="true" href="#training-curves">#</a></h2>
<h3 id="cumulative-rewards-in-order-of-best-performing-in-results-to-worst">Cumulative Rewards (In order of best performing in results to worst)<a hidden class="anchor" aria-hidden="true" href="#cumulative-rewards-in-order-of-best-performing-in-results-to-worst">#</a></h3>
<p>The cumulative reward curves reveal distinct learning dynamics across the three training configurations.</p>
<h4 id="hard-weighted-best">Hard-Weighted (Best)<a hidden class="anchor" aria-hidden="true" href="#hard-weighted-best">#</a></h4>
<p><img alt="Hard-Weighted Cumulative Rewards" loading="lazy" src="/blog/images/best_mixed%203%20datatset%20training/rewards_cumulative.png"></p>
<h4 id="balanced-worse">Balanced (Worse)<a hidden class="anchor" aria-hidden="true" href="#balanced-worse">#</a></h4>
<p><img alt="Balanced Cumulative Rewards" loading="lazy" src="/blog/images/worse%203%20mixed%20dataset%20training/rewards_cumulative.png"></p>
<h4 id="hard-only-2-sets---worst">Hard-Only (2 sets - worst)<a hidden class="anchor" aria-hidden="true" href="#hard-only-2-sets---worst">#</a></h4>
<p><img alt="Hard-Only Cumulative Rewards" loading="lazy" src="/blog/images/two%20sets%20training%20dataset/rewards_cumulative.png"></p>
<hr>
<h3 id="per-reward-component">Per Reward Component<a hidden class="anchor" aria-hidden="true" href="#per-reward-component">#</a></h3>
<p>Breaking down the reward signal into its constituent components shows how each configuration balances accuracy, exploration, and format compliance.</p>
<h4 id="hard-weighted-best-1">Hard-Weighted (Best)<a hidden class="anchor" aria-hidden="true" href="#hard-weighted-best-1">#</a></h4>
<p><img alt="Hard-Weighted Component Contribution" loading="lazy" src="/blog/images/best_mixed%203%20datatset%20training/component_contribution.png"></p>
<h4 id="balanced-worse-1">Balanced (Worse)<a hidden class="anchor" aria-hidden="true" href="#balanced-worse-1">#</a></h4>
<p><img alt="Balanced Component Contribution" loading="lazy" src="/blog/images/worse%203%20mixed%20dataset%20training/component_contribution.png"></p>
<h4 id="hard-only-2-sets">Hard-Only (2 Sets)<a hidden class="anchor" aria-hidden="true" href="#hard-only-2-sets">#</a></h4>
<p><img alt="Hard-Only Component Contribution" loading="lazy" src="/blog/images/two%20sets%20training%20dataset/component_contribution.png"></p>
<hr>
<h2 id="results-training-dataset-distribution-and-self-aware-generalization">Results: Training Dataset Distribution and Self-Aware Generalization<a hidden class="anchor" aria-hidden="true" href="#results-training-dataset-distribution-and-self-aware-generalization">#</a></h2>
<h3 id="experimental-configurations">Experimental Configurations<a hidden class="anchor" aria-hidden="true" href="#experimental-configurations">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>Training Distribution</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Hard-Weighted (Best)</strong></td>
          <td>ClimateFEVER + VitaminC heavy, FEVER light</td>
          <td>Skewed toward difficult examples with easy anchors</td>
      </tr>
      <tr>
          <td><strong>Balanced (Worse)</strong></td>
          <td>Equal split across all 3 datasets</td>
          <td>Uniform representation</td>
      </tr>
      <tr>
          <td><strong>Hard-Only (2 Sets)</strong></td>
          <td>ClimateFEVER + VitaminC only</td>
          <td>No easy examples</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="training-dynamics-reward-curves">Training Dynamics: Reward Curves<a hidden class="anchor" aria-hidden="true" href="#training-dynamics-reward-curves">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>Hard-Weighted (Best)</th>
          <th>Balanced (Worse)</th>
          <th>Hard-Only (2 Sets)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Final Total Reward</strong></td>
          <td>+1.0</td>
          <td>+2.8</td>
          <td>-1.7</td>
      </tr>
      <tr>
          <td><strong>r_explore (final)</strong></td>
          <td>+1.0</td>
          <td>+1.5</td>
          <td>~0</td>
      </tr>
      <tr>
          <td><strong>r_correct trajectory</strong></td>
          <td>Gradual climb</td>
          <td>Steep climb</td>
          <td>Flat (no learning)</td>
      </tr>
      <tr>
          <td><strong>Variance</strong></td>
          <td>Moderate</td>
          <td>High</td>
          <td>High</td>
      </tr>
  </tbody>
</table>
<p><strong>Hard-Weighted (Best)</strong>: The reward curve shows steady, moderate growth. r_correct climbs gradually from -0.6 to +0.8, and r_explore increases to +1.0. The total reward crosses zero around step 75 and stabilizes at +1.0. This gradual trajectory indicates stable learning without over-optimization.</p>
<p><strong>Balanced (Worse)</strong>: Despite achieving the highest training rewards (+2.8 total), the steep climb in r_correct (+1.8) and r_explore (+1.5) suggests aggressive optimization on the training distribution. The model learns to maximize the reward signal but overfits to the balanced mixture rather than developing robust cross-domain self-awareness.</p>
<p><strong>Hard-Only (2 Sets)</strong>: Complete training failure. All reward components remain flat throughout 200 steps‚Äîr_explore stays at zero, indicating the model never attempts PASS/FAIL predictions. The total reward is stuck at -1.7, demonstrating NA collapse where the model defaults to the safe neutral prediction.</p>
<hr>
<h3 id="evaluation-performance-relative-gains-from-baseline">Evaluation Performance (Relative Gains from Baseline)<a hidden class="anchor" aria-hidden="true" href="#evaluation-performance-relative-gains-from-baseline">#</a></h3>
<h4 id="hard-weighted-best---skewed-toward-hard-examples">Hard-Weighted (Best) - Skewed toward hard examples<a hidden class="anchor" aria-hidden="true" href="#hard-weighted-best---skewed-toward-hard-examples">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Accuracy</th>
          <th>Macro F1</th>
          <th>NA_f1</th>
          <th>NA_recall</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FEVER</td>
          <td>+18.4% ‚úÖ</td>
          <td>+0.34 ‚úÖ</td>
          <td>+0.01 ‚ùå</td>
          <td>+0.01 ‚úÖ</td>
      </tr>
      <tr>
          <td>VitaminC</td>
          <td>+20.7% ‚úÖ</td>
          <td>+0.30 ‚úÖ</td>
          <td><strong>+0.16 ‚úÖ</strong></td>
          <td><strong>+0.14 ‚úÖ</strong></td>
      </tr>
      <tr>
          <td>ClimateFEVER</td>
          <td>+23.4% ‚úÖ</td>
          <td>+0.34 ‚úÖ</td>
          <td><strong>+0.37 ‚úÖ</strong></td>
          <td><strong>+0.31 ‚úÖ</strong></td>
      </tr>
  </tbody>
</table>
<h4 id="balanced-worse---equal-split-across-3-datasets">Balanced (Worse) - Equal split across 3 datasets<a hidden class="anchor" aria-hidden="true" href="#balanced-worse---equal-split-across-3-datasets">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Accuracy</th>
          <th>Macro F1</th>
          <th>NA_f1</th>
          <th>NA_recall</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FEVER</td>
          <td>+19.6% ‚úÖ</td>
          <td>+0.35 ‚úÖ</td>
          <td>+0.03 ‚úÖ</td>
          <td>+0.01 ‚úÖ</td>
      </tr>
      <tr>
          <td>VitaminC</td>
          <td>+13.6% ‚úÖ</td>
          <td>+0.22 ‚úÖ</td>
          <td><strong>-0.04 ‚ùå</strong></td>
          <td><strong>-0.03 ‚ùå</strong></td>
      </tr>
      <tr>
          <td>ClimateFEVER</td>
          <td>+13.5% ‚úÖ</td>
          <td>+0.22 ‚úÖ</td>
          <td><strong>+0.02 ‚ùå</strong></td>
          <td><strong>+0.01 ‚ùå</strong></td>
      </tr>
  </tbody>
</table>
<h4 id="hard-only-2-sets---no-easy-examples">Hard-Only (2 Sets) - No easy examples<a hidden class="anchor" aria-hidden="true" href="#hard-only-2-sets---no-easy-examples">#</a></h4>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Accuracy</th>
          <th>Macro F1</th>
          <th>NA_f1</th>
          <th>NA_recall</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>FEVER</td>
          <td><strong>-3.2% ‚úÖ</strong></td>
          <td><strong>+0.03 ‚ùå</strong></td>
          <td><strong>-0.17 ‚úÖ</strong></td>
          <td>+0.01 ‚úÖ</td>
      </tr>
      <tr>
          <td>VitaminC</td>
          <td>+27.1% ‚úÖ</td>
          <td>+0.31 ‚úÖ</td>
          <td>+0.36 ‚úÖ</td>
          <td>+0.57 ‚úÖ</td>
      </tr>
      <tr>
          <td>ClimateFEVER</td>
          <td>+29.7% ‚úÖ</td>
          <td>+0.27 ‚úÖ</td>
          <td>+0.60 ‚úÖ</td>
          <td>+0.89 ‚úÖ</td>
      </tr>
  </tbody>
</table>
<p><strong>Legend:</strong></p>
<ul>
<li>‚úÖ = Statistically significant (p &lt; 0.01)</li>
<li>‚ùå = Not statistically significant</li>
<li><strong>Bold</strong> = Key metrics supporting conclusion</li>
</ul>
<hr>
<h3 id="key-findings">Key Findings<a hidden class="anchor" aria-hidden="true" href="#key-findings">#</a></h3>
<h4 id="1-hard-weighted-distribution-yields-best-overall-generalization">1. Hard-Weighted Distribution Yields Best Overall Generalization<a hidden class="anchor" aria-hidden="true" href="#1-hard-weighted-distribution-yields-best-overall-generalization">#</a></h4>
<p>The configuration with more hard examples (ClimateFEVER + VitaminC) than easy (FEVER) achieves the most balanced performance across all datasets. Critically, NA_f1 and NA_recall improvements are <strong>statistically significant on both hard datasets</strong> (VitaminC: 0.50/0.36, ClimateFEVER: 0.47/0.36). The model learns to recognize genuine uncertainty because it encountered sufficient difficult cases during training.</p>
<h4 id="2-equal-distribution-overfits-to-easy-examples">2. Equal Distribution Overfits to Easy Examples<a hidden class="anchor" aria-hidden="true" href="#2-equal-distribution-overfits-to-easy-examples">#</a></h4>
<p>The balanced configuration achieves the highest training rewards (+2.8) and best FEVER performance (95.5%), but <strong>fails to learn calibrated NA predictions on hard datasets</strong>. NA_f1 and NA_recall improvements are not statistically significant on VitaminC (0.31/0.19) or ClimateFEVER (0.11/0.06). The model optimizes aggressively for the training signal but doesn&rsquo;t develop robust self-awareness for challenging domains.</p>
<h4 id="3-hard-only-training-breaks-easy-domain-performance">3. Hard-Only Training Breaks Easy Domain Performance<a hidden class="anchor" aria-hidden="true" href="#3-hard-only-training-breaks-easy-domain-performance">#</a></h4>
<p>Excluding easy examples entirely causes training collapse‚Äîthe reward curves show no learning. While evaluation shows strong hard-dataset accuracy (VitaminC 63.2%, ClimateFEVER 57.4%), the model <strong>regresses on FEVER</strong> (72.7%, down from 75.9% baseline).</p>
<hr>
<h3 id="summary-table">Summary Table<a hidden class="anchor" aria-hidden="true" href="#summary-table">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>Training Signal</th>
          <th>Easy (FEVER)</th>
          <th>Hard Datasets</th>
          <th>NA Calibration</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Hard-Weighted (all 3 datasets)</strong></td>
          <td>Moderate (+1.0)</td>
          <td>Strong (94.3%)</td>
          <td><strong>Best</strong> (54.1% avg)</td>
          <td><strong>Significant</strong></td>
      </tr>
      <tr>
          <td><strong>Balanced (all 3 datasets)</strong></td>
          <td>Highest (+2.8)</td>
          <td><strong>Best</strong> (95.5%)</td>
          <td>Worst (45.5% avg)</td>
          <td>Not significant</td>
      </tr>
      <tr>
          <td><strong>Hard-Only (2 datasets, without fever)</strong></td>
          <td>Failed (-1.7)</td>
          <td><strong>Regressed</strong> (72.7%)</td>
          <td>Good (60.3% avg)</td>
          <td>N/A</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>These results directly support the claim that <strong>diverse training datasets improve generalization, with harder examples requiring stronger representation than easier ones to effectively shift model behavior</strong>. The hard-weighted configuration outperforms the balanced split on challenging domains while maintaining strong easy-domain performance. However, <strong>training exclusively on difficult examples fails to transfer to simpler domains</strong>‚Äîthe hard-only configuration shows this failure dramatically, with FEVER accuracy regressing below baseline.</p>
<p>The optimal strategy for self-aware fact verification is to weight training data toward hard examples while retaining easy examples as learning anchors. This enables the model to develop calibrated uncertainty estimation across the full difficulty spectrum, rather than over-optimizing for one end of the distribution.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Kadavath et al. &ldquo;Language Models (Mostly) Know What They Know.&rdquo; 2022. arXiv:2207.05221</p>
<p>[2] Xiong et al. &ldquo;Can LLMs Express Their Uncertainty?&rdquo; 2023. arXiv:2305.18153</p>
<p>[3] Schuster et al. &ldquo;Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence.&rdquo; NAACL 2021. ACL Anthology</p>
<p>[4] VitaminC GitHub. github.com/TalSchuster/VitaminC</p>
<p>[5] Aly et al. &ldquo;FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured Information.&rdquo; 2021. arXiv:2106.05707</p>
<p>[6] DeepSeek-AI. &ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&rdquo; 2025. arXiv:2501.12948</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/blog/">Pranav on a Tangent</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
