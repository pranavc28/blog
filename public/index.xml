<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Pranav on a Tangent</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content on Pranav on a Tangent</description>
    <generator>Hugo -- 0.150.1</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Jan 2026 07:07:07 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dataset Distribution needed for Generalization: A case study in self-awareness</title>
      <link>http://localhost:1313/blog/posts/generalization-dataset-distribution/</link>
      <pubDate>Tue, 20 Jan 2026 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/blog/posts/generalization-dataset-distribution/</guid>
      <description>&lt;p&gt;&lt;em&gt;&amp;ldquo;In all things the middle state is to be praised.&amp;rdquo; - &lt;strong&gt;Aristotle&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research code:&lt;/strong&gt; &lt;a href=&#34;https://github.com/pranavc28/self-awareness-grpo&#34;&gt;https://github.com/pranavc28/self-awareness-grpo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM:&lt;/strong&gt; openai/gpt-oss-20b&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datasets:&lt;/strong&gt; &lt;a href=&#34;https://github.com/tdiggelm/climate-fever-dataset&#34;&gt;https://github.com/tdiggelm/climate-fever-dataset&lt;/a&gt;, &lt;a href=&#34;https://fever.ai/download/fever/train.jsonl&#34;&gt;https://fever.ai/download/fever/train.jsonl&lt;/a&gt;, &lt;a href=&#34;https://github.com/TalSchuster/VitaminC&#34;&gt;https://github.com/TalSchuster/VitaminC&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;claim&#34;&gt;Claim&lt;/h2&gt;
&lt;p&gt;Diverse training datasets improve generalization, with harder examples requiring stronger representation than easier ones to effectively shift model behavior. However, training exclusively on difficult examples fails to transfer to simpler domains—a mixture of both easy and hard examples is necessary for robust cross-domain performance.&lt;/p&gt;
&lt;p&gt;The domain considered is self-awareness. Follow my previous blogs for motivation and background.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Awareness Generalization in Large Language Models</title>
      <link>http://localhost:1313/blog/posts/self-awareness-generalization/</link>
      <pubDate>Mon, 05 Jan 2026 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/blog/posts/self-awareness-generalization/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Research code:&lt;/strong&gt; &lt;a href=&#34;https://github.com/pranavc28/self-awareness-grpo&#34;&gt;https://github.com/pranavc28/self-awareness-grpo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLM:&lt;/strong&gt; openai/gpt-oss-20b&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Datasets:&lt;/strong&gt; &lt;a href=&#34;https://github.com/tdiggelm/climate-fever-dataset&#34;&gt;https://github.com/tdiggelm/climate-fever-dataset&lt;/a&gt;, &lt;a href=&#34;https://fever.ai/download/fever/train.jsonl&#34;&gt;https://fever.ai/download/fever/train.jsonl&lt;/a&gt;, &lt;a href=&#34;https://github.com/TalSchuster/VitaminC&#34;&gt;https://github.com/TalSchuster/VitaminC&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;claims&#34;&gt;Claims&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GRPO with calibration-aware rewards can improve NA recall by &amp;gt;10% on in-domain fact verification tasks.&lt;/strong&gt; With effective Reinforcement Learning techniques, it is possible to make Large Language Models (LLMs) more self-aware, and better at letting users know when they cannot provide a high confidence response to a request. We will call such an outcome &amp;ldquo;not enough information/not applicable&amp;rdquo; (NA). Such an outcome will be crucial, as AI embeds itself in more workflows, to gain more trust in situations where an incorrect outcome such as PASS/FAIL will worsen the user experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tinkering with Generative UI</title>
      <link>http://localhost:1313/blog/posts/generative-ui-tinker/</link>
      <pubDate>Fri, 14 Nov 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/blog/posts/generative-ui-tinker/</guid>
      <description>&lt;p&gt;&lt;em&gt;&amp;ldquo;Form and function should be one, joined in a spiritual union&amp;rdquo; - Frank Lloyd Wright&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Research code: &lt;a href=&#34;https://github.com/pranavc28/generative-ui&#34;&gt;https://github.com/pranavc28/generative-ui&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dataset: &lt;a href=&#34;https://huggingface.co/datasets/cfahlgren1/react-code-instructions/&#34;&gt;https://huggingface.co/datasets/cfahlgren1/react-code-instructions/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;purpose&#34;&gt;Purpose&lt;/h2&gt;
&lt;p&gt;This blog explores how I fine-tuned Qwen, Qwen/Qwen3-30B-A3B, an open source model producing React code for a fraction of the cost of any of the top-tier AI research labs. I used &lt;a href=&#34;http://tinker-docs.thinkingmachines.ai/&#34;&gt;Tinker&lt;/a&gt; to fine-tune my own LLM, a product released by the team at &lt;a href=&#34;https://thinkingmachines.ai/&#34;&gt;Thinking Machines Lab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It did a great job, and produced generally usable React code at a better level that I expected it to from an online dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Thinking to Knowing: Using Natural Language Confidence From LLM Thought Processes</title>
      <link>http://localhost:1313/blog/posts/thought-engineering-self-awareness/</link>
      <pubDate>Sun, 19 Oct 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/blog/posts/thought-engineering-self-awareness/</guid>
      <description>&lt;p&gt;&amp;ldquo;And this is wisdom and temperance and self-knowledge — for a man to know what he knows, and what he does not know.&amp;rdquo; - Plato, Charmides&lt;/p&gt;
&lt;p&gt;&amp;ldquo;To say you know when you know, and to say you do not when you do not — that is knowledge.&amp;rdquo; - Confucius&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&#34;https://www.yash-sharma.com/&#34;&gt;Yash Sharma&lt;/a&gt; for a lot of valuable feedback on my idea and evaluation methodology&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Research code: &lt;a href=&#34;https://github.com/pranavc28/thought-engineering-and-self-awareness&#34;&gt;https://github.com/pranavc28/thought-engineering-and-self-awareness&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;claim&#34;&gt;Claim&lt;/h1&gt;
&lt;p&gt;Thought engineering techniques (overthinking and automated confidence refinement) improve multi-classification performance across all model architectures. The purpose of this blog is to explain these terms, and prove why this is true.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Temperature Sampling for OCR-VQA: Does It Matter?</title>
      <link>http://localhost:1313/blog/posts/temperature-ocr-vqa/</link>
      <pubDate>Sat, 27 Sep 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/blog/posts/temperature-ocr-vqa/</guid>
      <description>&lt;p&gt;Research code: &lt;a href=&#34;https://github.com/pranavc28/temperature-ocr-vqa&#34;&gt;https://github.com/pranavc28/temperature-ocr-vqa&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;definitions&#34;&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Temperature in LLMs&lt;/strong&gt; controls how predictable or exploratory a model’s outputs are. Low temperature = consistent and factual, good for precise tasks. High temperature = more diverse, good for creative tasks—but also riskier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Question Answering (VQA)&lt;/strong&gt; is about answering questions directly from images. For OCR tasks, like reading a book cover, VQA can outperform raw OCR because it focuses only on what’s asked (e.g., &lt;em&gt;“Who’s the author?”&lt;/em&gt;) instead of dumping every piece of text.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
